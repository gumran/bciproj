{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M7aXkZslWhsH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "import mne\n",
        "from scipy import signal\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, f1_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.neighbors import NearestCentroid as NCC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def filter(data): # Pass sampling rate as an argument\n",
        "    # median filter to remove artifacts\n",
        "    # data = signal.medfilt(data, kernel_size=3) # Uncomment if needed\n",
        "\n",
        "    # --- High-Pass Filter ---\n",
        "    hpf_cutoff = 1  # Hz - Recommended for ERPs to remove slow drift\n",
        "    hpf_order = 4     # Filter order (can be same or different from LPF)\n",
        "\n",
        "    # Design Butterworth HPF\n",
        "    # Note: fs/2 is the Nyquist frequency\n",
        "    b_high, a_high = signal.butter(hpf_order, hpf_cutoff / 100, btype='high')\n",
        "\n",
        "    # Apply HPF using zero-phase filtering\n",
        "    # Apply along the time axis (assuming channels x time or trials x channels x time)\n",
        "    data_hpf = signal.filtfilt(b_high, a_high, data, axis=-1)\n",
        "\n",
        "    # --- Low-Pass Filter ---\n",
        "    lpf_cutoff = 40.0  # Hz - Typical for ERPs\n",
        "    lpf_order = 4      # Filter order\n",
        "\n",
        "    # Design Butterworth LPF\n",
        "    b_low, a_low = signal.butter(lpf_order, lpf_cutoff / 100, btype='low')\n",
        "\n",
        "    # Apply LPF using zero-phase filtering to the high-passed data\n",
        "    filtered_data = signal.filtfilt(b_low, a_low, data_hpf, axis=-1)\n",
        "\n",
        "    return filtered_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z027oMSeWkt3"
      },
      "outputs": [],
      "source": [
        "eeg_channels = ['Fp1', 'Fp2', 'AF7', 'AF3', 'AF4', 'AF8', 'F7', 'F5', 'F3',\n",
        "    'F1', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT7', 'FC5', 'FC3', 'FC1', 'FCz',\n",
        "    'FC2', 'FC4', 'FC6', 'FT8', 'T7', 'C5', 'C3', 'C1', 'Cz', 'C2', 'C4',\n",
        "    'C6', 'T8', 'TP7', 'CP5', 'CP3', 'CP1', 'CPz', 'CP2', 'CP4', 'CP6',\n",
        "    'TP8', 'P7', 'P5', 'P3', 'P1', 'Pz', 'P2', 'P4', 'P6', 'P8', 'PO7',\n",
        "    'POz', 'P08', 'O1', 'O2']\n",
        "\n",
        "eog_channel = 'EOG' \n",
        "\n",
        "all_channels = eeg_channels + [eog_channel]\n",
        "all_labels = pd.read_csv('TrainLabels.csv').Prediction.values\n",
        "\n",
        "try:\n",
        "    all_epochs = np.load('all_epochs.npy')\n",
        "except:\n",
        "    all_epochs = [] # Store cleaned EEG epochs\n",
        "\n",
        "    # --- Loop through sessions ---\n",
        "    for filename in tqdm(os.listdir('train')):\n",
        "        session_data = pd.read_csv(os.path.join('train', filename))\n",
        "\n",
        "        # --- 1. Filter EEG and EOG Channels ---\n",
        "        print(f\"\\nProcessing {filename}...\")\n",
        "        for id in all_channels:\n",
        "            session_data[id] = filter(session_data[id].values)\n",
        "\n",
        "        # --- 2. Prepare Continuous Data for ICA ---\n",
        "        # Shape: (n_channels, n_timepoints)\n",
        "        eeg_data = session_data[eeg_channels].values.T\n",
        "        eog_data = session_data[eog_channel].values.reshape(1, -1) # Keep as 2D array\n",
        "\n",
        "        # Transpose for sklearn: (n_timepoints, n_features)\n",
        "        eeg_data_skl = eeg_data.T\n",
        "        eog_data_skl = eog_data.T\n",
        "\n",
        "        # --- 3. Standardize Data ---\n",
        "        scaler_eeg = StandardScaler()\n",
        "        eeg_data_scaled = scaler_eeg.fit_transform(eeg_data_skl)\n",
        "\n",
        "        scaler_eog = StandardScaler()\n",
        "        eog_data_scaled = scaler_eog.fit_transform(eog_data_skl) # Scale EOG separately\n",
        "\n",
        "        # --- 4. Fit ICA on EEG Data ---\n",
        "        n_components_ica = len(eeg_channels) # Use number of EEG channels as components\n",
        "        ica = FastICA(n_components=n_components_ica,\n",
        "                        random_state=42,\n",
        "                        whiten='unit-variance', # Recommended for FastICA\n",
        "                        max_iter=1000, # Increase iterations if convergence is an issue\n",
        "                        tol=0.001)     # Convergence tolerance\n",
        "\n",
        "        print(\"Fitting ICA...\")\n",
        "        sources_eeg = ica.fit_transform(eeg_data_scaled) # Shape: (n_timepoints, n_components_ica)\n",
        "        print(\"ICA fitting complete.\")\n",
        "\n",
        "        # --- 5. Identify EOG Component(s) via Correlation ---\n",
        "        print(\"Identifying EOG components...\")\n",
        "        correlations = []\n",
        "        eog_signal_flat = eog_data_scaled.flatten() # Ensure EOG is 1D for correlation\n",
        "\n",
        "        for i in range(sources_eeg.shape[1]):\n",
        "            component_signal = sources_eeg[:, i]\n",
        "            # Calculate Pearson correlation coefficient and p-value\n",
        "            corr, _ = pearsonr(eog_signal_flat, component_signal)\n",
        "            correlations.append(abs(corr)) # Use absolute correlation\n",
        "\n",
        "            eog_component_index = np.argmax(correlations)\n",
        "            max_corr = correlations[eog_component_index]\n",
        "\n",
        "            print(f\"Removing component {eog_component_index} with correlation: {max_corr:.4f}\")\n",
        "            sources_cleaned = sources_eeg.copy()\n",
        "            sources_cleaned[:, eog_component_index] = 0\n",
        "\n",
        "        # --- 7. Reconstruct Cleaned EEG Data ---\n",
        "        print(\"Reconstructing cleaned EEG data...\")\n",
        "        # Use inverse_transform to go from sources back to sensor space\n",
        "        eeg_data_cleaned_scaled = ica.inverse_transform(sources_cleaned)\n",
        "\n",
        "        # --- 8. Un-standardize Cleaned EEG Data ---\n",
        "        eeg_data_cleaned_unscaled = scaler_eeg.inverse_transform(eeg_data_cleaned_scaled)\n",
        "\n",
        "        # Prepare cleaned data for epoching (back to DataFrame structure)\n",
        "        eeg_data_to_epoch = pd.DataFrame(eeg_data_cleaned_unscaled, columns=eeg_channels, index=session_data.index)\n",
        "        print(\"EEG data cleaned.\")\n",
        "\n",
        "\n",
        "        # --- 9. Epoching\n",
        "        session_feedback_ids = session_data[session_data['FeedBackEvent'] == 1].index\n",
        "        session_epochs = []\n",
        "        print(f\"Epoching {len(session_feedback_ids)} events...\")\n",
        "        for idx in session_feedback_ids:\n",
        "            # Define epoch window relative to event index\n",
        "            start_idx = idx - 40  # 200ms before (40 samples at 200Hz)\n",
        "            end_idx = idx + 199 # 1000ms after (200 samples at 200Hz) -> inclusive index for .loc\n",
        "\n",
        "            # Define baseline window relative to event index\n",
        "            baseline_start_idx = idx - 40 # 200ms before\n",
        "            baseline_end_idx = idx      # Up to (but not including) the event\n",
        "\n",
        "            # Extract epoch data for EEG channels\n",
        "            epoch_data = eeg_data_to_epoch.loc[start_idx:end_idx].copy()\n",
        "\n",
        "            # Calculate baseline using the correct window from the *epoched* data\n",
        "            baseline_data = epoch_data.loc[baseline_start_idx:baseline_end_idx]\n",
        "            baseline_mean = baseline_data.mean(axis=0)\n",
        "\n",
        "            # Apply baseline correction\n",
        "            epoch_data -= baseline_mean\n",
        "\n",
        "            # Append the baseline-corrected epoch (EEG channels only)\n",
        "            session_epochs.append(epoch_data.values) # Append numpy array\n",
        "\n",
        "        all_epochs.extend(session_epochs) # Add the cleaned epochs to the list\n",
        "        print(f\"Finished {filename}. Total epochs collected: {len(all_epochs)}\")\n",
        "\n",
        "\n",
        "    # --- Convert cleaned epochs to NumPy array ---\n",
        "    # Shape: (n_epochs, n_timepoints_in_epoch, n_eeg_channels)\n",
        "    all_epochs = np.array(all_epochs)\n",
        "    print(f\"Final shape of cleaned epochs array: {all_epochs.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MjXmoWz5WqW3"
      },
      "outputs": [],
      "source": [
        "epochs_train, epochs_test, y_train, y_test = train_test_split(all_epochs, all_labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L99OUEqsW6R2"
      },
      "source": [
        "# All time points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_dataset(epochs, channels, t_start, t_end, window_size):\n",
        "    time_points = range((t_start + 200) // 5, (t_end + 200) // 5, window_size)\n",
        "    X = []\n",
        "    for id in tqdm(range(len(channels))):\n",
        "        channel_data = [epoch[:, id] for epoch in epochs]  # shape: (n_epochs, n_timepoints)\n",
        "        channel_data = np.array(channel_data)  # shape: (n_epochs, 240)\n",
        "\n",
        "        for time_point in time_points:  # all time points from -200 to 1000 ms\n",
        "            time_point_data = channel_data[:, time_point:time_point+window_size].mean(axis=1)  # shape: (n_epochs,)\n",
        "            X.append(time_point_data)\n",
        "\n",
        "    X = np.array(X).T  # shape: (n_samples, n_features)\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yaa-yPcIbMQF"
      },
      "outputs": [],
      "source": [
        "def preprocess(X_train, X_test, apply_pca, n_components):\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    if not apply_pca:\n",
        "        print(\"No PCA applied\")\n",
        "        return X_train_scaled, X_test_scaled\n",
        "    \n",
        "    pca = PCA(n_components)  # This will retain enough components to explain 95% of the variance\n",
        "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "    X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "    print(f\"Applying PCA ({n_components if n_components else 1} variance, {pca.n_components_} components)\")\n",
        "    return X_train_pca, X_test_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cJrId9kbkzG"
      },
      "outputs": [],
      "source": [
        "def fit_models(X_train, X_test, y_train, y_test, apply_pca=False, n_components=0.95):\n",
        "    X_train_pca, X_test_pca = preprocess(X_train, X_test, apply_pca, n_components)\n",
        "    models = {\"NCC\": NCC(), \"LDA\": LDA(), \"RLDA\": LDA(solver='lsqr', shrinkage=\"auto\")}\n",
        "    for name in models.keys():\n",
        "        model = models[name]\n",
        "        model.fit(X_train_pca, y_train)\n",
        "        y_pred = model.predict(X_test_pca)\n",
        "        print(f\"    {name}: {f1_score(y_test, y_pred, average='macro'):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loop(X_train, X_test, y_train, y_test):\n",
        "    for apply_pca, n_components in [(False, None), (True, 0.95), (True, 0.99), (True, None)]:\n",
        "        fit_models(X_train, X_test, y_train, y_test, apply_pca=apply_pca, n_components=n_components)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 56/56 [00:01<00:00, 35.07it/s]\n",
            "100%|██████████| 56/56 [00:00<00:00, 199.84it/s]\n"
          ]
        }
      ],
      "source": [
        "X_train = build_dataset(epochs_train, eeg_channels, -200, 1000, 1)\n",
        "X_test = build_dataset(epochs_test, eeg_channels, -200, 1000, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No PCA applied\n",
            "    NCC: 0.5690\n",
            "    LDA:: 0.5670\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "loop(X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJs4U47vXJT-"
      },
      "source": [
        "# Interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def roc_matrix(epochs, channels, t_start, t_end, window_size):\n",
        "    time_points = range((t_start + 200) // 5, (t_end + 200) // 5, window_size)\n",
        "    channel_to_roc = {}\n",
        "    for id, channel in tqdm(enumerate(channels)):\n",
        "        channel_data = [epoch[:, id] for epoch in epochs]  # shape: (n_epochs, n_timepoints)\n",
        "        channel_data = np.array(channel_data)  # shape: (n_epochs, 240)\n",
        "\n",
        "        for time_point in time_points:  # all time points from -200 to 1000 ms\n",
        "            time_point_data = channel_data[:, time_point:time_point+window_size].mean(axis=1)  # shape: (n_epochs,)\n",
        "            roc = roc_auc_score(y_train, time_point_data)  # Calculate AUC for this time point\n",
        "            rocs.append(roc)\n",
        "        channel_to_roc[channel] = rocs\n",
        "    X = np.array(X).T  # shape: (n_samples, n_features)\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt-mMHLkXMhm"
      },
      "outputs": [],
      "source": [
        "# Step 2: Compute ROC-AUC for each channel using only the training data\n",
        "channel_to_roc = {}\n",
        "\n",
        "for id, channel in tqdm(enumerate(eeg_channels)):\n",
        "    # Get the channel data for training epochs only\n",
        "    channel_data_train = [epoch[:, id] for epoch in epochs_train]\n",
        "    channel_data_train = np.array(channel_data_train)  # Shape: (n_train_epochs, n_timepoints)\n",
        "\n",
        "    print(f\"Channel {channel} - channel_data_train shape: {channel_data_train.shape}\")\n",
        "\n",
        "    rocs = []\n",
        "\n",
        "    # Loop over each time point (no averaging, calculate AUC for each time point)\n",
        "    for time_point in range(channel_data_train.shape[1]):  # Looping over time points directly\n",
        "        time_point_data = channel_data_train[:, time_point]  # Get the data for the current time point\n",
        "        try:\n",
        "            roc = roc_auc_score(y_train, time_point_data)  # Calculate AUC for this time point\n",
        "            rocs.append(roc)\n",
        "        except ValueError as e:\n",
        "            print(f\"Error at time_point {time_point} for channel {channel}: {e}\")\n",
        "\n",
        "    # Store the ROC-AUC scores for this channel\n",
        "    channel_to_roc[channel] = rocs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_x_p6URYOLn"
      },
      "outputs": [],
      "source": [
        "# Step 3: Plot the ROC-AUC heatmap for training data\n",
        "arr = np.array([channel_to_roc[channel] for channel in eeg_channels])\n",
        "\n",
        "# Generate time labels from -200 ms to 1000 ms, rounded to integers\n",
        "time_labels = np.linspace(-200, 1000, arr.shape[1]).astype(int)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(arr, cmap='jet', vmin=0.35, vmax=0.65)\n",
        "\n",
        "# Adjust xticks to show every 5th timepoint for better readability\n",
        "plt.xticks(np.arange(0, arr.shape[1], 5), time_labels[::5], rotation=90)\n",
        "plt.yticks(np.arange(0, len(eeg_channels)), eeg_channels, rotation=0)\n",
        "\n",
        "# Labeling the axes and title\n",
        "plt.xlabel('Time (ms)')\n",
        "plt.ylabel('Channels')\n",
        "plt.title('ROC AUC for each channel over time (Training Data)')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEB6QMt9YQ8W"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------\n",
        "# 1. Load EEG Channel Locations\n",
        "# --------------------------------------------\n",
        "df = pd.read_csv(\"dataset/ChannelsLocation.csv\")\n",
        "\n",
        "# Convert spherical (Radius, Phi) -> Cartesian (X, Y, Z)\n",
        "df[\"Phi\"] = np.deg2rad(df[\"Phi\"])  # Degrees → Radians\n",
        "df[\"X\"] = df[\"Radius\"] * np.cos(df[\"Phi\"])\n",
        "df[\"Y\"] = df[\"Radius\"] * np.sin(df[\"Phi\"])\n",
        "df[\"Z\"] = np.zeros(len(df))  # Assume 2D electrode layout\n",
        "\n",
        "# Build Montage\n",
        "ch_pos = {row[\"Labels\"]: [row[\"X\"], row[\"Y\"], row[\"Z\"]] for _, row in df.iterrows()}\n",
        "montage = mne.channels.make_dig_montage(ch_pos, coord_frame=\"head\")\n",
        "\n",
        "# Create MNE Info object\n",
        "ch_names = list(df[\"Labels\"])\n",
        "info = mne.create_info(ch_names, sfreq=200, ch_types=\"eeg\")\n",
        "info.set_montage(montage)\n",
        "\n",
        "# --------------------------------------------\n",
        "# 2. Load ROC AUC values\n",
        "# --------------------------------------------\n",
        "# Assuming `channel_to_roc` and `eeg_channels` are already defined\n",
        "arr = np.array([channel_to_roc[channel] for channel in eeg_channels])  # Shape: (n_channels, n_timepoints)\n",
        "\n",
        "# Time labels: from -200 ms to 1000 ms\n",
        "n_timepoints = arr.shape[1]\n",
        "time_labels = np.linspace(-200, 1000, n_timepoints).astype(int)\n",
        "#time_labels = np.linspace(-200, 1000, 240)\n",
        "\n",
        "\n",
        "# --------------------------------------------\n",
        "# 3. Plot in Chunks\n",
        "# --------------------------------------------\n",
        "chunk_size = 48\n",
        "n_chunks = int(np.ceil(n_timepoints / chunk_size))\n",
        "\n",
        "for chunk in range(n_chunks):\n",
        "    start = chunk * chunk_size\n",
        "    end = min((chunk + 1) * chunk_size, n_timepoints)\n",
        "\n",
        "    f, ax = plt.subplots(6, 8, figsize=(22, 20))\n",
        "    ax = ax.ravel()\n",
        "\n",
        "    for i in range(start, end):\n",
        "        ax_idx = i - start\n",
        "        ax[ax_idx].set_title(f\"{time_labels[i]} ms\")\n",
        "        mne.viz.plot_topomap(\n",
        "            arr[:, i], info, size=3, cmap=\"jet\", vlim=(0.35, 0.65),\n",
        "            show=False, outlines=\"head\", axes=ax[ax_idx], contours=False\n",
        "        )\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for j in range(end - start, 48):\n",
        "        ax[j].axis(\"off\")\n",
        "\n",
        "    plt.suptitle(f\"ROC AUC Topomaps (Time: {time_labels[start]} to {time_labels[end - 1]} ms)\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(top=0.92)  # Adjust space for suptitle\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "hmPCBMVsYbcv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/56 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 56/56 [00:00<00:00, 107.69it/s]\n",
            "100%|██████████| 56/56 [00:00<00:00, 422.56it/s]\n"
          ]
        }
      ],
      "source": [
        "features = [f\"{channel} voltage at {time_point} ms\"\n",
        "            for channel in eeg_channels for time_point in range(300, 450, 5)]\n",
        "\n",
        "X_train = build_dataset(epochs_train, eeg_channels, 300, 450, 1)\n",
        "X_test = build_dataset(epochs_test, eeg_channels, 300, 450, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "5qqpTx1xdC99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No PCA applied\n",
            "    NCC: 0.5618\n",
            "    LDA:: 0.6018\n",
            "    RLDA: 0.6090\n",
            "\n",
            "Applying PCA (0.9 variance, 18 components)\n",
            "    NCC: 0.5625\n",
            "    LDA:: 0.5040\n",
            "    RLDA: 0.4877\n",
            "\n",
            "Applying PCA (0.95 variance, 41 components)\n",
            "    NCC: 0.5618\n",
            "    LDA:: 0.5282\n",
            "    RLDA: 0.5263\n",
            "\n",
            "Applying PCA (0.975 variance, 96 components)\n",
            "    NCC: 0.5618\n",
            "    LDA:: 0.5561\n",
            "    RLDA: 0.5438\n",
            "\n",
            "Applying PCA (1 variance, 1680 components)\n",
            "    NCC: 0.5618\n",
            "    LDA:: 0.5961\n",
            "    RLDA: 0.5789\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loop(X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "janxGTtwXN2-"
      },
      "source": [
        "# Windowed Interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "CYv2gWEcXSYG"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 56/56 [00:00<00:00, 99.60it/s] \n",
            "100%|██████████| 56/56 [00:00<00:00, 372.45it/s]\n"
          ]
        }
      ],
      "source": [
        "features = [f\"{channel} voltage at {time_point} ms\"\n",
        "            for channel in eeg_channels for time_point in range(300, 450, 5)]\n",
        "\n",
        "X_train = build_dataset(epochs_train, eeg_channels, 300, 450, 5)\n",
        "X_test = build_dataset(epochs_test, eeg_channels, 300, 450, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No PCA applied\n",
            "    NCC: 0.5660\n",
            "    LDA:: 0.6262\n",
            "    RLDA: 0.5750\n",
            "\n",
            "Applying PCA (0.9 variance, 8 components)\n",
            "    NCC: 0.5645\n",
            "    LDA:: 0.4801\n",
            "    RLDA: 0.4770\n",
            "\n",
            "Applying PCA (0.95 variance, 16 components)\n",
            "    NCC: 0.5645\n",
            "    LDA:: 0.5154\n",
            "    RLDA: 0.5046\n",
            "\n",
            "Applying PCA (0.975 variance, 29 components)\n",
            "    NCC: 0.5648\n",
            "    LDA:: 0.5458\n",
            "    RLDA: 0.5320\n",
            "\n",
            "Applying PCA (1 variance, 336 components)\n",
            "    NCC: 0.5660\n",
            "    LDA:: 0.6262\n",
            "    RLDA: 0.6196\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loop(X_train, X_test, y_train, y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
